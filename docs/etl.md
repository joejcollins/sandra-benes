# ETL Pipeline

crawl > normalize > chunk > embed > index

## Extraction

Gathering into `raw`.
I am going to skip this step.
Actually it can be pretty complicated especially for webpages.
Getting the page to execute with

## Parsing/Normalisation

Converting into "standard" ready for database into `processed`

in our case get the other data from the file (filename)

where does the chunking go?

Does the embedding happen here too?
